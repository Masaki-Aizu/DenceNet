# DenseNet
## ポイント
- DenseBlockを導入。Block内の畳み込み層上位から下位まですべての層にSkipConnetctionを導入
- Block内のSkipConnectionの総数は **L(L+1)/2 個**  
- DenseNet には、利点が4つある。1,勾配消失問題を軽減し、2,特徴の伝播を強化し、3,特徴の再利用を促進し、4,パラメーターの数を大幅に削減します。
## DenseBlock
- Block内のすべてのレイヤー (特徴マップサイズが一致する) を相互に直接接続する 
- 各層は先行するすべての層から追加の入力を取得し、独自の特徴マップを後続のすべての層に渡す 
図 1 は、このレイアウトを概略的に示しています。 重要なのは、ResNets とは対照的に、機能がレイヤーに渡される前に、合計によって機能を結合することはありません。
代わりに、機能を連結することで機能を組み合わせます。 したがって、l 層には l 個の入力があり、先行するすべての畳み込みブロックの特徴マップで構成されます。 
それ自体の特徴マップは、すべての L− l 個の後続の層に渡されます。 これにより、従来のアーキテクチャのように単なる L ではなく、L 層ネットワークに L(L+1)/2 接続が導入されます。
その高密度な接続パターンのため、私たちはこのアプローチを高密度畳み込みネットワーク (DenseNet) と呼んでいます。
この高密度接続パターンのおそらく直観に反する効果は、冗長な特徴マップを再学習する必要がないため、必要なパラメーターが従来の畳み込みネットワークよりも少ないことです。
従来のフィードフォワード アーキテクチャは、層から層へと受け渡される状態を持つアルゴリズムとみなすことができます。 各層は、前の層から状態を読み取り、後続の層に書き込みます。
状態を変更しますが、保存する必要がある情報も渡します。 ResNets [11] は、追加の ID 変換を通じてこの情報の保存を明示的にします。
ResNets の最近のバリエーション [13] では、多くの層がほとんど貢献しておらず、実際にはトレーニング中にランダムに削除される可能性があることが示されています。 
これにより、ResNet の状態は (アンロールされた) リカレント ニューラル ネットワーク [21] に似たものになりますが、各層が独自の重みを持っているため、ResNet のパラメーターの数はかなり多くなります。
私たちが提案する DenseNet アーキテクチャは、ネットワークに追加される情報と保存される情報を明確に区別します。
DenseNet 層は非常に狭く (たとえば、層ごとに 12 個のフィルタ)、少数の特徴マップ セットのみをネットワークの「集合知識」に追加し、残りの特徴マップは変更しないでおきます。
そして、最終的な分類器はすべての特徴に基づいて決定を行います。 -ネットワーク内のマップ。
DenseNets の大きな利点の 1 つは、パラメーター効率の向上に加えて、ネットワーク全体の情報の流れと勾配が改善され、トレーニングが容易になることです。 
各層は損失関数と元の入力信号からの勾配に直接アクセスでき、暗黙的な深い監視が可能になります [20]。 これは、より深いネットワーク アーキテクチャのトレーニングに役立ちます。
さらに、密な接続には正則化効果があり、トレーニング セット サイズが小さいタスクでの過剰適合が減少することも観察されています。
## 参考
1. https://arxiv.org/pdf/1608.06993.pdf
